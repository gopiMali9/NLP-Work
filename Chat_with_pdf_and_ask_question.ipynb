{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K4iPcJNP1ycm"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install langchain openai\n",
        "!pip3 install openai\n",
        "!pip install pdfminer.six\n",
        "!pip install chromadb\n",
        "!pip install tiktoken\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RykbdU3p15B2",
        "outputId": "bb1ef809-9bef-483a-b975-c50bf07a9b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0.201\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain import document_loaders\n",
        "from langchain import text_splitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.vectorstores.base import VectorStoreRetriever\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hHweU3_9AAGI"
      },
      "outputs": [],
      "source": [
        "# Clip Forge research paper CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation (CVPR 2022) from AutodeskAILab\n",
        "documents = document_loaders.PDFMinerLoader(\"/content/2110.02624.pdf\")\n",
        "# documents.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XPdqLkvm2T5L"
      },
      "outputs": [],
      "source": [
        "loaded_docs = documents.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FRU6uZIxBgGf"
      },
      "outputs": [],
      "source": [
        "loaded_docs[0].page_content = loaded_docs[0].page_content.replace('\\t', ' ').replace('\\n', ' ')\\\n",
        "                                                      .replace('       ', ' ').replace('      ', ' ')\\\n",
        "                                                      .replace('     ', ' ').replace('    ', ' ')\\\n",
        "                                                      .replace('   ', ' ').replace('  ', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IPqpXL0Cbb6",
        "outputId": "a7fcea9d-2e9f-4aaf-8ca0-a3fd800b8be2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# len(loaded_docs[0].page_content)//3\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(loaded_docs)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FV9IonxpDoH2"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=\"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\" ,)\n",
        "vectorstore = Chroma.from_documents(documents=texts,embedding=embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1, \"search_type\" : \"similarity\"})\n",
        "\n",
        "retrievalQA = RetrievalQA.from_chain_type(llm=OpenAI(max_tokens=1000,temperature=0,openai_api_key=\"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\"),\n",
        "                                   retriever=retriever ,chain_type=\"stuff\",\n",
        "                                   return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9GgtOi4LT3n",
        "outputId": "9b9170e0-2fd1-4aaf-88cc-d1b5cbbf2a80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------\n",
            "Ans : -  A voxel encoder is a type of encoder used in an autoencoder. It consists of a series of batch-normalized 3D convolution layers followed by linear layers. It is used to extract the shape embedding en for the training shape collection, using Vn of resolution 323 as the input.\n",
            "------------------------------------------------------------\n",
            "Document No. is [Document(page_content='Autoencoder The autoencoder consists of an encoder and a decoder. We use an encoder fV to extract the shape embedding en for the training shape collection, using Vn of resolution 323 as the input. We use a simple voxel network that comprises of a series of batch-normalized 3D convolution layers fol- lowed by linear layers. This can be written as: en = fV (Vn) + ϵ, where ϵ ∼ N (0, 0.1) (1) where en is augmented with a Gaussian noise. We find em- pirically injecting this noise improves the generation quality as later shown in the ablation study. This is also theoreti- cally verified to improve results for conditional density es- timation [52]. We then pass en through an implicit decoder. Our decoder architecture is inspired by the Occupancy Net- works [37], which takes concatenated en and Pn as input. Our implicit decoder consists of linear layers with residual connections and predicts On. We use a mean squared error loss between the predicted occupancy and the ground truth occupancy.', metadata={'source': '/content/2110.02624.pdf'})]\n"
          ]
        }
      ],
      "source": [
        "query = \"what is voxel encoder\"\n",
        "llm_response = retrievalQA(query)\n",
        "print(\"-\"*60)\n",
        "print(\"Ans : -\" , llm_response['result'])\n",
        "print(\"-\"*60)\n",
        "print(f\"Document No. is {llm_response['source_documents']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
